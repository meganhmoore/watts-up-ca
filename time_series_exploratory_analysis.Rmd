---
title: "Watts Up CA: Models"
author: "Claire Boyd, Kathryn Link-Oberstar, Megan Moore, Eshan Prasher"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r}
suppressMessages({
  library(readxl)
  library(dplyr)
  library(forecast)
  library(tidyverse)
  library(tsibble)
  library(fpp)
  library(tseries)
  library(tidyr)
})

file_path <- 'data/sorted_electricity_consumption_data.xlsx'
sorted_data <- read_excel(file_path)
residential <- ts(as.numeric(sorted_data$RESIDENTIAL.Sales.Megawatthours), start = c(1990, 1), frequency = 12)
```

## SUPPLEMENTARY DATA

### Monthly Temperature Data for all of California

```{r, results='hold'}
# Load the Monthly Temperature Data

temp_data <- read_excel('./data/ca_monthly_temp_data.xlsx')
temp_ts <- ts(temp_data$Value, frequency = 12, start=c(1990,1))
plot(temp_ts, ylab="Temperature", main="Monthly Average Temperature 1990-2023")
```

```{r, results=TRUE}
plot(decompose(temp_ts))
```

With the temperature data we do not seem to have a notable upward or downward trend, though from the decomposition it might be experiencing a slight positive trend. We see a consistent 12-month seasonal trend that seems to be additive in nature given that the amplitude of the variance is not growing from the looks of it.

Read in minimum temperatures.

```{r}
min_temp <- read_csv(url("https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series/4/tmin/all/1/1990-2023.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000"), skip = 4)

min_temp_ts <- ts(min_temp$`Value`, frequency = 12, start =c(1990,1), end=c(2023, 7))

plot(min_temp_ts)
```

Read in maximum temperatures.

```{r}
max_temp <- read_csv(url("https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series/4/tmax/all/1/1990-2023.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000"), skip = 4)

max_temp_ts <- ts(max_temp$`Value`, frequency = 12, start =c(1990,1), end=c(2023, 7)) # ending in July 2023 when consumption data ends

plot(max_temp_ts)
```

### Yearly Population Data

```{r}
pop_data <- read_excel('./data/ca_pop_1990_2022.xlsx', sheet="Data")
pop_ts <- ts(pop_data[2], frequency = 1, start=1990)
plot(pop_ts, ylab="Population (millions)", main="Population of California 1990-2022")
```

For the population data we see an upward trend that may be leveling off (would maybe need damping) , or this may be due to the effects of covid and will continue linearly increasing. There is no seasonality as these are annual population estimates.

### Unemployment Data

```{r}
unemp_data <- read_excel('./data/bls_cali_seasonal_adj_1990_2023.xlsx', 
                         sheet="Sheet1")

unemp_rate_ts <- ts(unemp_data$`unemployment rate`, frequency = 12, start =c(1990,1))
plot(unemp_rate_ts, ylab="Unemployment rate", main="Monthly unemployment rate in
     California 1990-2023")
```

\*\*\* General observations: - Almost at its lowest levels pre-Covid and post 2022 - Before Covid, peak was in 2009 - after the financial crisis of 2008. However, then, double-digit unemployed stayed till 2012 which hasn't happened after Covid. - Before 2008 recession, near double digit unemployement in 1992-93 due to national recession - job cuts in manufacturing

```{r}
plot(decompose(unemp_rate_ts))
```

## TRAIN TEST SPLITS

```{r}
#train/test split at 1990
train_res_1990 <- window(residential, start=1990, end=c(2022, 6))
test_res_1990 <- window(residential, start=c(2022, 7), end=c(2023, 7))
train_test_res_1990 <- window(residential, start=1990, end=c(2023, 7))

#train/test split at 2005
train_res_2005 <- window(residential, start=2005, end=c(2022, 6))
test_res_2005 <- window(residential, start=c(2022, 7), end=c(2023, 7))
train_test_res_2005 <- window(residential, start=1990, end=c(2023, 7))

#train/test split at 2013
train_res_2013 <- window(residential, start=2013, end=c(2022, 6))
test_res_2013 <- window(residential, start=c(2022, 7), end=c(2023, 7))
train_test_res_2013 <- window(residential, start=2013, end=c(2023, 7))
```

### Train-Test Splits for Independent Variables

```{r}
split_time_series <- function(time_series, year) {
  start_train <- c(year, 1)
  end_train <- c(2022, 6)
  start_test <- c(2022, 7)
  end_test <- c(2023, 7)
  start_train_test <- c(year, 1)
  end_train_test <- c(2023, 7)
  
  train_set <- window(time_series, start = start_train, end = end_train)
  test_set <- window(time_series, start = start_test, end = end_test)
  train_test_set <- window(time_series, start = start_train_test, end = end_train_test)
  
  return(list(train = train_set, test = test_set, train_test = train_test_set))
}
```

```{r}
# train test split for temp data
min_1990 <- split_time_series(min_temp_ts, 1990) 
train_min_temp_1990 <- min_1990$train
test_min_temp_1990 <- min_1990$test
train_test_min_temp_1990 <- min_1990$train_test

min_2005 <- split_time_series(min_temp_ts, 2005) 
train_min_temp_2005 <- min_2005$train
test_min_temp_2005 <- min_2005$test
train_test_min_temp_2005 <- min_2005$train_test

min_2013 <- split_time_series(min_temp_ts, 2013) 
train_min_temp_2013 <- min_2013$train
test_min_temp_2013 <- min_2013$test
train_test_min_temp_2013 <- min_2013$train_test

max_1990 <- split_time_series(max_temp_ts, 1990) 
train_max_temp_1990 <- max_1990$train
test_max_temp_1990 <- max_1990$test
train_test_max_temp_1990 <- max_1990$train_test

max_2005 <- split_time_series(max_temp_ts, 2005) 
train_max_temp_2005 <- max_2005$train
test_max_temp_2005 <- max_2005$test
train_test_max_temp_2005 <- max_2005$train_test

max_2013 <- split_time_series(max_temp_ts, 2013) 
train_max_temp_2013 <- max_2013$train
test_max_temp_2013 <- max_2013$test
train_test_max_temp_2013 <- max_2013$train_test
```

## BASELINE MODELS

### Seasonal Naive baseline model

The point forecasts are the same for each of these models, but the information criteria estimates are different (and the confidence interval bands are different for each)

MAPE for 1990: 5.521388 RMSE for 1990: 562980.8

MAPE for 2005: 6.094906 RMSE for 2005: 657427

MAPE for 2013: 7.692256 RMSE for 2013: 797692.8

Both are increasing as we narrow the window size.

```{r}
snaive_model_1990 = snaive(train_res_1990, lambda='auto', h=12)
accuracy(snaive_model_1990)

snaive_model_2005 = snaive(train_res_2005, lambda='auto', h=12)
accuracy(snaive_model_2005)

snaive_model_2013 = snaive(train_res_2013, lambda='auto', h=12)
accuracy(snaive_model_2013)

plot(snaive_model_2013)
```

```{r}
Acf(snaive_model_1990$residuals, lag=100)
Pacf(snaive_model_1990$residuals, lag=100)
```

### Exponential Smoothing

model type: ETS(M,N,M) MAPE for 1990: 4.667398 RMSE for 1990: 466188.8

model type: ETS(M,N,A) MAPE for 2005: 5.300409 RMSE for 2005: 535969.3

model type: ETS(M,N,M) MAPE for 2013: 5.55343 RMSE for 2013: 573835.4

Weird that with different windows a different ETS model is being specified. Similar to seasonal naive, as we increase the size of training data, we increase the MAPE and RSME values.

```{r}
ets_model_1990 = ets(train_res_1990,lambda='auto')
summary(ets_model_1990)

ets_model_2005 = ets(train_res_2005,lambda='auto')
summary(ets_model_2005)

ets_model_2013 = ets(train_res_2013,lambda='auto')
summary(ets_model_2013)
```

```{r}
plot(forecast(ets_model_1990, h=12))
```

### Linear Regression

MAPE for 1990: 6.877424 RMSE for 1990: 610919.6

MAPE for 2005: 5.426142 RMSE for 2005: 533482.7

MAPE for 2013: 5.572336 RMSE for 2013: 570245.5

Unlike the other baselines, this slightly improves with time and then gets worse again.

```{r}
tslm_model_1990 = tslm(train_res_1990 ~ trend + season)
summary(tslm_model_1990)
accuracy(tslm_model_1990)

tslm_model_2005 = tslm(train_res_2005 ~ trend + season)
summary(tslm_model_2005)
accuracy(tslm_model_2005)

tslm_model_2013 = tslm(train_res_2013 ~ trend + season)
summary(tslm_model_2013)
accuracy(tslm_model_2013)
```

```{r}
Acf(tslm_model_1990$residuals)
pacf(tslm_model_1990$residuals)
```

```{r}
plot(forecast(tslm_model_1990, h=12))
```

```{r}
tslm_model_1990
```

### Auto Arima

```{r}
residential_auto_arima_1990 <- auto.arima(train_res_1990, 
                                     seasonal = TRUE, 
                                     allowdrift = TRUE, 
                                     lambda = 'auto')

summary(residential_auto_arima_1990)
checkresiduals(residential_auto_arima_1990)
accuracy(residential_auto_arima_1990)
plot(forecast(residential_auto_arima_1990, h=12))
```

-   Residuals are not white noise

```{r}
residential_auto_arima_2005 <- auto.arima(train_res_2005, 
                                     seasonal = TRUE, 
                                     allowdrift = TRUE, 
                                     lambda = 'auto')
summary(residential_auto_arima_2005)
checkresiduals(residential_auto_arima_2005)
accuracy(residential_auto_arima_2005)
plot(forecast(residential_auto_arima_2005, h=12))
```

```{r}
residential_auto_arima_2013 <- auto.arima(train_res_2013, 
                                     seasonal = TRUE, 
                                     allowdrift = TRUE, 
                                     lambda = 'auto')
summary(residential_auto_arima_2013)
checkresiduals(residential_auto_arima_2013)
accuracy(residential_auto_arima_2013)
plot(forecast(residential_auto_arima_2013, h=12))
```

```{r}
checkresiduals(residential_auto_arima_2013)
```

## MODEL 1: REGRESSION

### Linear Regression with temperature and unemployment

First, creating a dataframe with all variables from 1990-2022

```{r}
# Create a sequence of years and months from 1990 to 2022
dates <- seq(as.Date("1990-01-01"), as.Date("2022-12-01"), by="months")

# Create a dataframe with Year and Month columns
residential_reg_df <- data.frame(Year = as.integer(format(dates, "%Y")), 
                                 Month = as.integer(format(dates, "%m")))

# Extract yearly population values from the pop_data dataframe
residential_reg_df$pop <- as.numeric(rep(pop_data$population, each = 12))

# Extract consumption data from sorted_data
residential_reg_df$elec_cons <- as.numeric(subset(sorted_data, Year >= 1990 & 
                                        Year <= 2022)$RESIDENTIAL.Sales.Megawatthours)

# Extract temperature data from temp_data
# First, we create a new 'Year' column by extracting the first four characters from 'Date'

temp_data$Year <- as.numeric(substr(temp_data$Date, 1, 4))
residential_reg_df$temp <- as.numeric(subset(temp_data,Year >= 1990 & 
                                        Year <= 2022)$Value)

# Extract unemployment rate data from unemp_data
residential_reg_df$unemp_rate <- as.numeric(subset(unemp_data, Year >= 1990 & 
                                              Year <= 2022)$`unemployment rate`)

# Calculate consumption per capita by dividing consumption by population
# consumption is MWhrs per million population
residential_reg_df$cons_capita <- residential_reg_df$elec_cons/(residential_reg_df$pop)

# Optionally, you can set row names if needed
rownames(residential_reg_df) <- NULL

# Print the first few rows to verify
head(residential_reg_df, 15)
```

```{r}
# Adding normalized values of 'temperature' and 'unemployment rate' to the dataframe 
residential_reg_df$norm_temp <- scale(residential_reg_df$temp)
residential_reg_df$norm_unemp_rate <- scale(residential_reg_df$unemp_rate)

# First, we will regress only with temperature
y_var <- ts(residential_reg_df$cons_capita, start = c(1990,1), frequency = 12)
x1_var <- ts(residential_reg_df$temp, start = c(1990,1), frequency = 12)
  
tslm_residential_temp <- tslm(y_var ~ x1_var, lambda = 'auto')  

# Summarize the regression results
summary(tslm_residential_temp)
accuracy(tslm_residential_temp)


# Now we add unemployment to this
x2_var <- ts(residential_reg_df$unemp_rate, start = c(1990,1), frequency = 12)
tslm_residential_temp_unemp <- tslm(y_var ~ x1_var + x2_var, lambda = 'auto') 

# Summarize the regression results
summary(tslm_residential_temp_unemp)
accuracy(tslm_residential_temp_unemp)

# Then just regress with population 
x3_var <- ts(residential_reg_df$unemp_rate, start = c(1990,1), frequency = 12)
tslm_residential_pop <- tslm(y_var ~ x3_var, lambda = 'auto')  

# Summarize the regression results
summary(tslm_residential_pop)
accuracy(tslm_residential_pop)
```

\*\*\* Key takeaways: - temperature is statistically significant, unemployment rate is not - maybe look at other labour market variables (or normalize and check), population is also not statistically significant indicating that consumption increases are not necessarily attributable to more people using electricity.

-   Do we look at adjusted R\^2? why not?

-   forecast is not working on this for some reason - need to debug - I think this is due to the fact that we need to know the "future" independent variables

```{r}
# plot(forecast(tslm_residential_pop), h=12)
# TODO forecast 
# 1) train model on only training set 
# 2) set temp/pop/whatever is being regressed on aside for test set
# 3) figure out how to provide the x_vals for the forcast
```

```{r}
### Experimenting with other regressions

y_var <- ts(residential_reg_df$cons, start = c(1990,1), frequency = 12) # consumption instead of consumption per capita

tslm_residential_tot_temp <- tslm(y_var ~ x1_var, lambda = 'auto') 

accuracy(tslm_residential_tot_temp)
summary(tslm_residential_tot_temp)

# Takeaway: No impact of standardizing consumption by population in previous regression
```

\*\*\* Next step: combine Claire's autoregression with temperature (and any other variable) to create models discussed in Lecture-7

## MODEL 2: REGRESSION WITH ARMA ERRORS

```{r}
residential_reg_w_arima_err_1990 <- auto.arima(train_res_1990, 
                                               xreg = cbind(train_min_temp_1990, train_max_temp_1990),
                                               seasonal = TRUE) # using lambda and drift made it look pretty whack

residential_reg_w_arima_err_2005 <- auto.arima(train_res_2005, 
                                               xreg = cbind(train_min_temp_2005, train_max_temp_2005),
                                               seasonal = TRUE) # using lambda and drift made it look pretty whack

residential_reg_w_arima_err_2013 <- auto.arima(train_res_2013, 
                                               xreg = cbind(train_min_temp_2013, train_max_temp_2013),
                                               seasonal = TRUE) # using lambda and drift made it look pretty whack
```

```{r}
summary(residential_reg_w_arima_err_1990)
summary(residential_reg_w_arima_err_2005)
summary(residential_reg_w_arima_err_2013)
checkresiduals(residential_reg_w_arima_err_1990)
checkresiduals(residential_reg_w_arima_err_2005)
checkresiduals(residential_reg_w_arima_err_2013)
```

```{r}
residential_reg_w_arima_err_1990_for <- forecast(residential_reg_w_arima_err_1990, h=12, xreg=cbind(test_min_temp_1990, test_max_temp_1990))

autoplot(residential_reg_w_arima_err_1990_for) +
  autolayer(test_res_1990)

residential_reg_w_arima_err_2005_for <- forecast(residential_reg_w_arima_err_2005, h=12, xreg=cbind(test_min_temp_2005, test_max_temp_2005))

autoplot(residential_reg_w_arima_err_2005_for) +
  autolayer(test_res_2005)

residential_reg_w_arima_err_2013_for <- forecast(residential_reg_w_arima_err_2013, h=12, xreg=cbind(test_min_temp_2013, test_max_temp_2013))

autoplot(residential_reg_w_arima_err_2013_for) +
  autolayer(test_res_2013)

plot(residential_reg_w_arima_err_1990_for)
plot(residential_reg_w_arima_err_2005_for)
plot(residential_reg_w_arima_err_2013_for)
```

## MODEL 3: VECTOR AUTOREGRESSIONS (VAR and VARIMA)

## MODEL TESTING

*Models to Test*

```{r}
models <- list(
  list("Residential Auto Arima - 1990", residential_auto_arima_1990, train_res_1990, test_res_1990, 12),
  list("Residential Auto Arima - 2005", residential_auto_arima_2005, train_res_2005, test_res_2005, 12),
  list("Residential Auto Arima - 2013", residential_auto_arima_2013, train_res_2013, test_res_2013, 12),
  list("Residential Seasonal Naive - 1990", snaive_model_1990, train_res_1990, test_res_1990, 12),
  list("Residential Seasonal Naive - 2005", snaive_model_2005, train_res_2005, test_res_2005, 12),
  list("Residential Seasonal Naive - 2013", snaive_model_2013, train_res_2013, test_res_2013, 12),
  list("ETS - 1990", ets_model_1990, train_res_1990, test_res_1990, 12),
  list("ETS - 2005", ets_model_2005, train_res_2005, test_res_2005, 12),
  list("ETS - 2013", ets_model_2013, train_res_2013, test_res_2013, 12),
  list("Linear Regression - 1990", tslm_model_1990, train_res_1990, test_res_1990, 12),
  list("Linear Regression - 2005", tslm_model_2005, train_res_2005, test_res_2005, 12),
  list("Linear Regression - 2013", tslm_model_2013, train_res_2013, test_res_2013, 12),
  list("Regression with ARIMA Errors - 1990", residential_reg_w_arima_err_1990, train_res_1990, test_res_1990, 12, cbind(test_min_temp_1990, test_max_temp_1990)), 
  list("Regression with ARIMA Errors - 2005", residential_reg_w_arima_err_2005, train_res_2005, test_res_2005, 12, cbind(test_min_temp_2005, test_max_temp_2005)),
  list("Regression with ARIMA Errors - 2013", residential_reg_w_arima_err_2013, train_res_2013, test_res_2013, 12, cbind(test_min_temp_2013, test_max_temp_2013))
)
```

*Training Metrics*

```{r}
evaluate_models_train <- function(models) {
  results <- list()
  
  for (model_info in models) {
    
    model_name <- model_info[[1]]
    model <- model_info[[2]]
    train_data <- model_info[[4]]
    test_data <- model_info[[4]]
    horizon <- model_info[[5]]
    
    errors <- accuracy(model)
    MAPE <- errors["Training set", "MAPE"]
    RMSE <- errors["Training set", "RMSE"]
    AICc <- ifelse(exists("aicc", where = model), model$aicc, NA)
    
    results[[model_name]] <- list(MAPE = MAPE, RMSE = RMSE, AICc = AICc)

    cat(paste0(model_name, " Model Performance on Training Data: \n",
               "MAPE: ", MAPE, "\n",
               "RMSE: ", RMSE, "\n",
               "AICc: ", AICc, "\n\n"))
  }

  return(results)
}

results_training <- evaluate_models_train(models)
```

*Testing Metrics*

```{r}
evaluate_models_test <- function(models) {
  results <- list()
  
  for (model_info in models) {
    
    model_name <- model_info[[1]]
    model <- model_info[[2]]
    train_data <- model_info[[4]]
    test_data <- model_info[[4]]
    horizon <- model_info[[5]]
    if (length(model_info) >= 6){
      xreg_val <- model_info[[6]]
      model_forecast <- forecast(model, xreg=xreg_val, h=horizon)
    } else {
      model_forecast <- forecast(model, h=horizon)
    }
    
    errors <- accuracy(model_forecast, test_data)
    MAPE <- errors["Test set", "MAPE"]
    RMSE <- errors["Test set", "RMSE"]
    AICc <- ifelse(exists("aicc", where = model), model$aicc, NA)
    
    results[[model_name]] <- list(MAPE = MAPE, RMSE = RMSE, AICc = AICc, Forecast = model_forecast)
    
    cat(paste0(model_name, " Model Performance on Test Data: \n",
               "MAPE: ", MAPE, "\n",
               "RMSE: ", RMSE, "\n",
               "AICc: ", AICc, "\n\n"))
  }
  
  return(results)
}

results <- evaluate_models_test(models)
```
